---
title: 'Homework 1'
subtitle: ''
author: 'Daria Dubovskaia'
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: united
  pdf_document:
    toc: yes
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: sentence
---


```{r setup, include=FALSE}
# chunks
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, include=TRUE, 
message=FALSE, warning=FALSE, fig.height=5, fig.align='center')

# libraries
library(ranger)
library(glmnet)
library(pROC)
library(recipes)
library(viridis)
library(tidyverse)
library(doParallel)
library(kableExtra)
library(caTools)
library(reshape)
library(summarytools)
library(gridExtra)
library(superml)
library(MASS) # glm.nb()
library(mice)
library(lubridate)
library(skimr)
library(sjPlot)
library(mpath)
library(yardstick)
library(mlr)
library(randomForest)
library(LiblineaR)
library(labelled)
library(haven)
library(corrplot)
library(Hmisc)
library(jtools)
library(glmnet)
library(caret)
library(broom)

library(treemap)
library(ggplot2)
library(hrbrthemes)
library(viridis)
library(rpart)
library(rpart.plot)
library(vip)

library(ggpubr)
library(rockchalk)
library(nnet)


# random seed
set.seed(42)

Sys.setlocale("LC_ALL","English")
```



## Overview


A sales strategy’s success and efficiency can be influenced by a variety of elements, including product type, geography, sales channel, seasonality, etc. Understanding which sales to prioritize in the fast-paced world of sales can dramatically optimize resource allocation and increase overall sales performance. Sales with a high priority often necessitate fast response and can result in large income or customer retention.

Predicting the priority of sales in the large terrain of sales activities, particularly in a global setting, can be difficult. A sale’s priority level can be influenced by factors such as the region of sale, the type of goods, the sales channel(online/offline), and even the time of year.

This study dived deeply into the prediction of sales prioritization. We hoped to forecast whether a specific sale will have a vital priority by analyzing small data of 1,000 sales and large data of 50,000 sales. The feature variables included a variety of characteristics such as the sale region, item type, sales channel, units sold, their cost, and time-related parameters such as the month and year of the sale.

As the target variable was binary, we built two machine learning models: Logistic Regression and Random Forest Regression. Our data was split into training and testing sets using an 80/20 split. We deployed our revised models on a training dataset following model training with testing dataset and internal validation, assuring an unbiased assessment of our prediction prowess for both datasets (small and large)

------------------------------------------------------------------------

## 1. Data Preparation

```{r data_load}
# load data
df_1000 <- read.csv("https://raw.githubusercontent.com/ex-pr/DATA_622/main/HW%201/1000%20Sales%20Records.csv")
df_50000 <- read.csv("https://raw.githubusercontent.com/ex-pr/DATA_622/main/HW%201/50000%20Sales%20Records.csv")
``` 



### 1.1 Summary Statistics
The **small** dataset contained **1000** observations of 17 predictor variables.  

The **large** dataset contained **50000** observations over the same predictor variables. 

Each record in both datasets described a single sales transaction in full, including the product type, sales channel, financial data, and pertinent dates. Specifically:

-    `Region`: Geographical region where the sale occurred (e.g. Asia, Europe, North America, etc.)
-    `Country`: The country where the sale took place
-    `Item Type`: Type of product sold (e.g. Cereal, Cosmetics, Fruits, etc.)
-    `Sales Channel`: The method through which the sale was made (offline/online)
-    `Order Priority`: The order's priority (e.g., M for medium, C for critical, H for high, etc.)
-    `Order Date`: The date the order was placed
-    `Order ID`:A unique identifier for the order
-    `Ship Date`: The date the product was shipped
-    `Units Sold`: The number of units sold
-    `Unit Price`:The price per unit of the product 
-    `Unit Cost`: The cost per unit of the product
-    `Total Revenue`: The total revenue from the sale
-    `Total Cost`: The total cost of the transaction
-    `Total Profit`:  The total profit from the sale

The target variable `Critical Priority` would be deployed from the `Order Priority`. The value of the target would be 1 for a critical order priority, 0 for a medium, low or high priority.

As the target variable was binary, the following algorithms were considered. The following data preparation was based on this algorithm selection. The choice of the algorithms wasn't affected by the size of the data, only by the nature of the target variable.

`Logistic regression`: simple, interpretable, and efficient, when the relationship independent variables and log chances are linear, it performs well, but may underperform when the data has complex, non-linear relationships.

`Random Forest`: can capture complicated, non-linear patterns in data, resistant to overfitting, can handle numerical as well as categorical data. But it is more difficult to interpret than simpler models, large datasets can be computationally expensive.

```{r head_data_small}
DT::datatable(
      df_1000[1:25,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

```{r head_data_large}
DT::datatable(
      df_50000[1:25,],
      extensions = c('Scroller'),
      options = list(scrollY = 350,
                     scrollX = 500,
                     deferRender = TRUE,
                     scroller = TRUE,
                     dom = 'lBfrtip',
                     fixedColumns = TRUE, 
                     searching = FALSE), 
      rownames = FALSE) 
```

The table below provided a summary statistics for both datasets. There were no missing values in both.

Variables `Units Sold`, `Total Cost`, `Total Profit` had a wide range and could be a source of potential issue when building the models, scaling would be a solution for these variables.

It would be expected if the 50000 sales data would provide more variability and more unique values in categorical columns, but there were almost the same mean and median values for `Units Sold`, `Total Cost`, `Total Profit` and `Total Profit` in both datasets.

The small and large datasets used in the assignment, they both could introduce errors:

`Large Data`: could lead to overfitting. In this case, we might consider using regularization and validation techniques.

`Small Data`: could lead to underfitting. We also could get a high variance in predictions.

```{r}
print(dfSummary(df_1000, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE), headings = FALSE, max.tbl.height = 300, footnote = NA, col.width=50, method="render")
```

```{r summary_large}
print(dfSummary(df_50000, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE), headings = FALSE, max.tbl.height = 300, footnote = NA, col.width=50, method="render")
```

### 1.2 Column change

To make the dataset easy to work with when building models or conducting EDA, the series of steps were made starting with changing the name of the columns to lower case and substituting dot with underscore.

```{r col_name_function}
# A function to change the column names in a data frame to lower case and substituting dot with underscore

change_column_names <- function(df) {
  # Get the column names of the data frame
  
  colnames(df) <- colnames(df)  %>%
  str_to_lower()  %>%
  str_replace_all("\\.", "_")
  
  # Return the updated data frame
  return(df)
}
```

```{r col_name}
#Change column names, lower case, substitute dot with _
df_1000 <- change_column_names(df_1000)
df_50000 <- change_column_names(df_50000)
```

The `Order Date` or `Ship Date` may be important when building the model. These columns were transformed to the appropriate date format (yyyy/mm/dd).

```{r to_date_function}

# A function to transform a character columns with date in the format "yyyy/mm/dd" to a date format

	transform_date_columns <- function(df) {
  # Get the column names 
  col_names <- colnames(df)
  
  # Go through each column in the dataframe
  for (col_name in col_names) {
    # Check if this is character column 
    if (is.character(df[[col_name]])) {
      # Check if the column values are in the format dd/mm/yyyy
      if (any(grepl("^\\d{2}/\\d{2}/\\d{4}$", df[[col_name]]))) {
        # Transform the column values to the date format, %Y to show years 2000–2068 properly
        df[[col_name]] <- as.Date(df[[col_name]], format="%m/%d/%Y")
      }
    }
  }
  
  # Return the updated dataframe
  return(df)
	}

```

```{r to_date}
# Transform the character columns with dates to a date format
df_1000 <- transform_date_columns(df_1000)
df_50000 <- transform_date_columns(df_50000)
```

The `Country` column  was removed as it could be presented by `Region`. The `Order ID` column was removed as well as it was a unique number for a sale and it wasn't needed for the models.

```{r del_country_id}
# Delete columns country and order_id
df_1000 <- df_1000 %>%
          dplyr::select(-c(country, order_id))

df_50000 <- df_50000 %>%
          dplyr::select(-c(country, order_id))
```

### 1.3 New columns

New factor column: 

-    `critical_priority`: Target variable from `Order Priority`. If the order priority was critical , the value was 1, 0 for the rest.

New numeric columns:

-     `ship_time`: The shipping duration. The difference between `Ship Date` and `Order Date`. It could be connected to the order priority.
-     `year`: Extracted year from the `Orer Date`. It could affect the order priority.
-     ` month` Extracted month from the `Orer Date`. It could be affect the order priority.

Columns `Order Date`, `Ship Date`, `Order Priority` were removed as now we have dummy variables that are easy to work with instead.

Some other columns were also created as a try-out. For example, binary `is_weekend`, the value if the variable was 1 if the day of the order was weekend,0 otherwise. Or the column `quarter` to determine in which quarter of the year the order was placed. The new columns didn't change the performance of the model and were removed to avoid overfitting the models.

```{r col_quarter_function}
#   A function to add a new column "quarter" based on a date column
#   The quarter of the year corresponding to each date

#	add_quarter_column <- function(df, date_col) {
  # Check if the date column is in the data frame
	  
  #if (!(date_col %in% colnames(df))) {
   # stop("No such date column.")
 # }
  
  # Extract the quarter from the date column using lubridate::quarter function
  #df$quarter <- factor(quarter(df[[date_col]]))
  
  # Return the updated data frame with the new "quarter" column
  #return(df)
#}
```

```{r col_weekend_function}
#   A function to add a dummy variable indicating if the day of the order date
#   was a weekend (the value 1) or not (the value 0)

#add_weekend_indicator <- function(df, date_col) {
   # Check if the specified column exists
#    if (!(date_col %in% colnames(df))) {
#    stop("No such date column.")
 # }

  # Add a new dummy variable 'is_weekend'
#  df$is_weekend <- factor(ifelse(weekdays(df[[date_col]]) %in% c("Saturday", "Sunday"), 1, 0))
  
  # Return the updated dataframe
#  return(df)
#}

```

```{r col_online_function}
# A function to add a dummy variable indicating if the sales channel was a online, the value in the new column 
# would be 1, otherwise 0


#add_online_indicator <- function(df, channel_column) {
  # Check if the specified column exists 
#  if (!(channel_column %in% colnames(df))) {
#    stop("No such date column.")
#  }
  
  # Add a new dummy variable 'is_online'
#  df$is_online <- factor(ifelse(df[[channel_column]] == "Online", 1, 0))
  
  # Return the updated data frame
#  return(df)
#}
```


```{r new_columns}
# Create new column ship_time

df_1000$ship_time <- as.numeric(difftime(df_1000$ship_date, df_1000$order_date, units = "days"))

df_50000$ship_time <- as.numeric(difftime(df_50000$ship_date, df_50000$order_date, units = "days"))
```

```{r add_month_year}
# Extract month and year from order date 
df_1000$order_month <- month(df_1000$order_date)
df_1000$order_year <- year(df_1000$order_date)

df_50000$order_month <- month(df_50000$order_date)
df_50000$order_year <- year(df_50000$order_date)
```

```{r add_critical_priority}
# Create 'critical_priority' column based on 'Order Priority', remove date columns
df_1000$critical_priority <- ifelse(df_1000$order_priority == "C", '1', '0')


df_1000 <- df_1000 %>%
    dplyr::select(-c(order_priority, ship_date, order_date))

df_50000$critical_priority <- ifelse(df_50000$order_priority == "C", '1', '0')

df_50000 <- df_50000 %>%
    dplyr::select(-c(order_priority, ship_date, order_date))
```


### 1.4 Change column type

Character columns were transformed to factor to be used in the models (e.g. region, item type) as well as target variable (critical_priority).

```{r to_factor_function}
# A function to transform all character columns to factors

transform_character_columns <- function(df) {
  # Identify the character columns in the data frame
  char_cols <- sapply(df, is.character)
  
  # Transform the character columns to factor columns
  df[char_cols] <- lapply(df[char_cols], as.factor)
  
  # Return the updated data frame
  return(df)
}
```

```{r to_factor}
# Transform to factor character columns
df_1000 <- transform_character_columns(df_1000)
df_50000 <- transform_character_columns(df_50000)
```


## 2. Data Exploration

The structure and columns of both datasets were the same. The resulting datasets contained 13 variables.

The data contained inherent dependencies. For example, `total_revenue` could be calculated by multiplying `units_sold` and `unit_price`. Or `total_profit` could be calculated by subtracting `total_cost` from `total_revenue`. 

There was no specific label for supervised learning. However, we created a new variable from `order_priority` called `critical_priority` to predict if the order was of a critical priority based on other features. 

The consistent structure facilitated a uniform analysis.

```{r summary_small_update}
print(dfSummary(df_1000, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE), headings = FALSE, max.tbl.height = 300, footnote = NA, col.width=50, method="render")

```

```{r summary_large_update}
print(dfSummary(df_50000, text.graph.col = FALSE, graph.col = FALSE, style = "grid", valid.col = FALSE), headings = FALSE, max.tbl.height = 300, footnote = NA, col.width=50, method="render")
```

### 2.1 Numeric varibles

`Units Sold`: Uniform distribution, the majority of sales records appeared to be centered around the 5,000 level, although there was a broad range, with some records having extremely high or extremely low unit sales.

`Unit Price`,`Unit Cost`: The distribution was multi-modal, showing that things were priced and cost differently.

`Ship Time`: The majority of shipping dates appeared to be centered around the 25 days, although there was a broad range.

`Total Revenue`: There was a right skew, suggesting that the majority of sales records generated revenue of less than $2.5 million, but there were a few records with significantly larger revenue.

`Total Cost`: The distribution was right-skewed, as was Total Revenue.

`Total Profit`: Right-skewed, the distribution shows that the majority of profit values were under $1 million, however there were records with significantly larger profit.

The distribution of numeric values were similar for the small and large data.

```{r density_plot_function}
# A function to create histograms and density plots for numeric columns.
# The column names were change, underscore was subsitute with spaces, the first letter of each word was capitalized for a better representation on the plots
# Histograms and density plots are then created for each numeric variable and displayed using facet_wrap.
# Add title for each data frame

density_plot_function <-function(df, title) {
  
  # Select numeric columns and rename them
  m_df <- df %>% dplyr::select_if(is.numeric) %>% rename_all(~str_replace_all(., "_", " ") %>%
                tools::toTitleCase()) %>% melt()

   # Create histograms with the density ggplot
  m_df %>% ggplot() + 
  geom_histogram(aes(x=value, y = ..density..), alpha=0.7, fill="gray", colour='gray') +
    geom_density(aes(x=value), color='purple', size=1) + facet_wrap(~variable, scales = 'free',  ncol = 3) + 
    theme_minimal()+
    labs(title=title, x = 'Variables', y = 'Values') 
}
```

```{r density_plot}
# Create density plots for numeric columns
density_plot_function(df_1000, 'Distributions for 1000 Sales Records')
density_plot_function(df_50000, 'Distributions for 50000 Sales Records')
```


`Total Revenue` vs. `Units Sold`: There was a strong positive association between the number of units sold and the total revenue earned by those sales, showing that as the number of units sold increases, so did the overall revenue. This was understandable given that income is directly proportional to the number of units sold multiplied by the unit price.

`Total Revenue` vs. `Unit Price`: There was no obvious linear relationship between total income and item unit pricing. While higher unit prices could lead to higher revenues, the quantity sold also played a role. Some lower-priced items might have higher sales quantities, resulting in equivalent income.

The scatter plots were similar for the small and large data.

```{r scatter_plots_revenue_function}
# A function to create scatter plots to visualize a grid:
# Units Sold vs Total Revenue
# Unit Price vs Total Revenue
# Total Cost vs Total Profit
# 

plot_relationships <- function(df, title) {
  # Plot for Units Sold vs Total Revenue
  a1 <- ggplot(df, aes(x=units_sold, y=total_revenue)) +
    geom_point(alpha=0.5, color="blue") +
    labs(
        x = "Unit Sold",
        y = "Total Revenue") +
    theme_minimal()
  
  # Plot for Unit Price vs Total Revenue
  a2 <- ggplot(df, aes(x=unit_price, y=total_revenue)) +
    geom_point(alpha=0.5, color="blue") +
    labs(
        x = "Unit Price",
        y = "Total Revenue") +
    theme_minimal()
  
  # Plot for Total Cost vs Total Profit
  a3 <- ggplot(df, aes(x=total_cost, y=total_profit)) +
    geom_point(alpha=0.5, color="blue") +
    labs(
        x = "Total Cost",
        y = "Total Profit") +
    theme_minimal()
  
  # Arrange the plots in a grid
  grid.arrange(a1, a2, a3, ncol=3, top=title)
}
```

```{r scatter_plots_revenue}
# Scatter plots for numeric columns
plot_relationships(df_1000, 'Relationships for 1000 Sales Records')
plot_relationships(df_50000, 'Relationships for 50000 Sales Records')
```

`Total Profit` vs. `Units Sold`: The scatter plot demonstrated an expected positive linear relationship between the number of `Units Sold` and `Total Profit`.

The `Order Priority` hue offered further information about how different order priority were spread among sales records. It was worth noting that higher order priorities didn't always equate to more units sold or bigger profitability.

The scatter plots were similar for the small and large data.

```{r scatter_plot_profit}
# Scatter plot for Units Sold vs Total Profit colored by Order Priority
ggplot(df_1000, aes(x=units_sold, y=total_profit, color=critical_priority)) +
    geom_point() +
    scale_color_viridis_d(option = "inferno") +       
    labs(
        x = "Units Sold",
        y = "Total Profit", 
        title = 'Units Sold vs Total Profit colored by Order Priority, 1000 sales') +
    theme_minimal()

ggplot(df_50000, aes(x=units_sold, y=total_profit, color=critical_priority)) +
    geom_point() +
    scale_color_viridis_d(option = "inferno") +       
    labs(
        x = "Units Sold",
        y = "Total Profit", 
        title = 'Units Sold vs Total Profit colored by Order Priority, 50000 sales') +
    theme_minimal()

```




### 2.3 Category variables


`Order Priority Count`: On the graph with the distribution of records based on order priority, the majority of orders had not a critical priority.

`Total Profit by Order Priority`: On the box plot with the distribution of `Total Profit` across different order priority, profits varied widely across priority, with the median profit appearing to be consistent.

`Units Sold by Order Priority`: There was no apparent trend demonstrating that a specific order priority regularly generated greater or lower sales. Although all priorities had a wide range of sales, the median sales appeared to be larger for non-critical priority.

The large data was more balanced in terms of dummy variables than small data.

```{r priority_function}

# A function to create 3 box plots that compare sales and order priority data:
# The number of orders by order priority.
# The total profit by order priority.
# The units sold by order priority.

priority_sales_function <- function(df, title) {

# Bar plot for 'Order Priority'
a2 <- ggplot(df, aes(x=critical_priority)) +
    geom_bar(fill=viridis(1)) + 
            labs(
        x = "Order Priority",
        y = "Count", 
        title = "'Count by Order Priority") +
    theme_minimal() + theme(legend.position = "none")


# Box plot for 'Total Profit' by 'Order Priority'
a4 <- ggplot(df, aes(x=critical_priority, y=total_profit, fill=critical_priority)) +
    geom_boxplot() +
    scale_fill_viridis_d() +
    ggtitle('Total Profit by Order Priority') +
              labs(
        x = "Order Priority",
        y = "Total Profit", 
        title = "'Total Profit by Order Priority") +
    theme_minimal() +  theme(legend.position = "none")


a6 <- ggplot(df, aes(x=critical_priority, y=units_sold, fill=critical_priority)) +
    geom_boxplot() +
    scale_fill_viridis_d() +       
        labs(
        x = "Order Priority",
        y = "Units Sold", 
        title = "Units Sold by Order Priority") +
    theme_minimal() +  theme(legend.position = "none")


# Print the plots
grid.arrange(a2, a4, ncol=2, top=title)
grid.arrange(a6, ncol=1)
}

```

```{r priority_sales_plot}
# Box plots for sales channel, order priority
priority_sales_function(df_1000, "Order Priority for 1000 sales")
priority_sales_function(df_50000, "Order Priority for 50000 sales")
```

Critical priority sales were distributed similarly across both online and offline sales channels, with non-critical sales being more common.
```{r}
# Distribution of Critical Priority by Sales Channel
ggplot(df_1000, aes(x = sales_channel, fill = critical_priority)) +
  geom_bar(position = "dodge") +
  labs(
    x = 'Sales Channel', 
    y = 'Count', 
    title = 'Critical Priority by Sales Channel, 1000 Sales') +
  theme_minimal() +  theme(legend.position = "none")

ggplot(df_50000, aes(x = sales_channel, fill = critical_priority)) +
  geom_bar(position = "dodge") +
  labs(
    x = 'Sales Channel', 
    y = 'Count', 
    title = 'Critical Priority by Sales Channel, 50000 Sales') +
  theme_minimal() +  theme(legend.position = "none")
```


`Count vs Region`: The region of Sub-Saharan Africa had the most sales records, and within this region, the number of sales with a critical priority was substantially smaller than those without. Europe and Central Africa had a significantly larger number of critical priority sales.

`Total Profit vs Region`: The median profit was reasonably consistent among regions, although each location had a large range of profits, as evidenced by the length of the boxes and whiskers.

`Units Sold by Region`: The median number of units sold was rather consistent among regions as shown on the box plot, similar to the profit distribution, however there was diversity within each zone.

All the plots for both data sets were almost the same except that median for units sold by region and total profit by region were more equal for a large data than for a small data.

```{r byregion_function}
# A function to create a set of three box plots that compare sales data by region:
# The number of orders by region.
# The total profit by region.
# The units sold by region.

byregion_function <- function(df, title) {
  # Reordering the levels of 'Region' based on their counts
  df$region <- factor(df$region, levels = names(sort(table(df$region), decreasing = TRUE)))

# Bar plot for 'Region'
a1 <- ggplot(df, aes(x = region, fill = critical_priority)) +
  geom_bar(position = "dodge") + 
      labs(
        x = "Region",
        y = "Count", 
        title = "Critical Priority by Region") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

# Box plot for 'Total Profit' by 'Region'
a2 <- ggplot(df, aes(x=region, y=total_profit, fill=region)) +
    geom_boxplot() +
    scale_fill_viridis_d() +
      labs(
        x = "Region",
        y = "Total Profit", 
        title = "Total Profit by Region") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

# Box plot for 'Units Sold' by 'Region'
a3 <- ggplot(df, aes(x=region, y=units_sold, fill=region)) +
    geom_boxplot() +
    scale_fill_viridis_d() +
        labs(
        x = "Region",
        y = "Units Sold", 
        title = "Units Sold by Region") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

grid.arrange(a1, ncol=1, top=title)
grid.arrange(a2, ncol=1)
grid.arrange(a3, ncol=1)
  
}



```

```{r byregion}
# Boxplots for Region disctribution
byregion_function(df_1000, "Distribution by Region, 1000 Sales")
byregion_function(df_50000, "Distribution by Region, 50000 Sales")
```

`Profitability by Item Type`: The distribution of `Total Profit` among different item types showed the variation in profit among item types. Some things, such as `Cosmetics` had larger median profits than others, such as `Vegetables`, which had lower median earnings.

`Units Sold by Item Type`: The distribution of `Units Sold` across different item kinds depicted that the number of units sold varied depending on the product. For example, `Baby Food` had a greater median unit sale, whereas 'Cereal' had a lower median.

`Count by Item Type`: Item types, cosmetics, household, and office supplies appear to have a higher number of critical priority sales. Fruits, vegetables, and snacks, on the other hand, have a smaller proportion of essential priority sales.



```{r profit_units_type_function}
# A function to create a set of two plots to compare the distribution of total profit and units sold by item type:
# The total profit by item type.
# Tthe units sold by item type.

items_distribution <- function(df, title) {
  # Reordering the levels of 'Item Type' based on their counts
df$item_type <- factor(df$item_type, levels = names(sort(table(df$item_type), decreasing = TRUE)))

# Box plot for 'Total Profit' by 'Item Type'
a1 <- ggplot(df, aes(x=item_type, y=total_profit, fill=item_type)) +
    geom_boxplot() +
    scale_fill_viridis_d() +
        labs(
        x = "Item Type",
        y = "Total Profit", 
        title = "Total Profit by Item Type") +
    theme_minimal()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

# Box plot for 'Units Sold' by 'Item Type'
a2 <- ggplot(df, aes(x=item_type, y=units_sold, fill=item_type)) +
    geom_boxplot() +
    scale_fill_viridis_d() +
        labs(
        x = "Item Type",
        y = "Units Sold", 
        title = "Units Sold by Item Type") +
    theme_minimal()+
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none")

# Distribution of Critical Priority by Item Type
a3 <- ggplot(df, aes(x = item_type, fill = critical_priority)) +
  geom_bar(position = "dodge") +
  labs(
    x ="'Item Type", 
    y = "Count", 
    title = 'Critical Priority by Item Type') +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


# Print the plots
grid.arrange(a1, ncol=1, top=title)
grid.arrange(a2, ncol=1)
grid.arrange(a3, ncol=1)
}
```

```{r profit_units_type}
# Boxplots for item distribution
items_distribution(df_1000, "Item Type disctribution for 1000 sales")
items_distribution(df_50000, "Item Type disctribution for 50000 sales")
```


`Count vs. Order Year`: The data ranging from 2010 to 2017, with the quantity of sales records fluctuating from year to year.
 
Next graph showed the monthly sales trend from 2010 to 2017. There appeared to be a repeating trend in sales, with some months typically seeing more sales than others. The magnitude of sales varies from year to year. Around most years, sales peak around the middle of the year, particularly around June and July.


```{r }
# Order Year count

ggplot(df_50000, aes(x = as.factor(order_year))) +
  geom_bar() + 
  labs(title = 'Order Year Distribution, 50000 Sales', x = 'Order Year', y = 'Count') +
  theme(text = element_text(size = 12),
        axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5, size = 15)) +
  theme_minimal() 

# Aggregate data by order year and order month
sales <- df_50000 %>%
  group_by(order_year, order_month) %>%
  summarise(sales = n()) %>%
  ungroup()

# Monthly Sales Trend
ggplot(sales, aes(x = order_month, y = sales, group = order_year, color = as.factor(order_year))) +
  geom_line(aes(linetype = as.factor(order_year))) +
  geom_point(shape = 21, fill = "white") +
  scale_x_continuous(breaks = 1:12, labels = c('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec')) +
  labs(title = 'Monthly Sales Trend (2010-2017)', x = 'Month', y = 'Count') +
  theme(text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5, size = 15),
        legend.position = "top",
        legend.title = element_text(size = 12)) +
  guides(color = guide_legend(title = "Year"))



```
### 2.3 Correlation 

The `Unit Price` and `Unit Cost` had a significant connection, which made logical given that cost and price were frequently associated.
`Total Revenue`, `Total Cost` and `Total Profit` were likewise closely connected, which was to be expected given that they were generated from one another.
`Units Sold` had a strong relationship with `Total Revenue`, `Total Cost`, and `Total Profit`. This made sense because the number of units sold had an immediate influence on revenue, cost, and profit.

`Region` and `Country` were be entirely dependent on one another for an obvious reason. As well as `Order Date` and `Shipping Date` that were naturally  correlated, and additionally affected by the item type.

The correlation matrix for 1000 sales.
```{r corr_small}

rcore <- rcorr(as.matrix(df_1000 %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```

The correlation matrix for 50000 sales.
```{r corr_large}

rcore <- rcorr(as.matrix(df_50000 %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```




## 3 Data Transformation

Finally, we split our data into train (80%) and test (20%) datasets to evaluate model performance before we proceeded to prediction. The train data contained 800 sales, test data 200 sales for a small dataset.

Slightly different variations were used depending on each model's needs.

The preprocessing technique and sample.split() were used for the simple Logistic Regression. The preprocessing consisted of encoding categorical variables (`region`, `item_type`, `sales_channel`) to make model's coefficients more interpretable. 

For the 1000 Sales Data, the response variable had a balance of 74% for `0` response and 26% for `1` response. This was the result of the distribution of the original column `Order Priority` where 4 values (C, H, M, L) had a part of ~26% each. It could happen that if we used undersampling or oversampling techniques for the target variable, the accuracy of our models could be 50%. For the further analysis, it could be an option to try balance the response variable.



```{r split_regression}
temp_1 <- df_1000


# Creating recipe to encode all categorical variables except for a target variable
preprocess_factors <- recipe(critical_priority ~ ., data = temp_1) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE)

# Fit the preprocessing technique 
preprocess_factors_prep <- prep(preprocess_factors)


# Encode factor variables
temp_1_transformed <- bake(preprocess_factors_prep, new_data = temp_1)
temp_1_transformed <- temp_1_transformed  %>%
  mutate_at(vars(starts_with("item_type"), starts_with("region"), starts_with("sales_channel")), as.factor)

# random seed
set.seed(42)

# 80/20 split of the data set
sample <- sample.split(temp_1_transformed$critical_priority, SplitRatio = 0.8)
train_data  <- subset(temp_1_transformed, sample == TRUE)
test_data   <- subset(temp_1_transformed, sample == FALSE)

# Check dimenstions of train and test data
dim(train_data)
dim(test_data)

# Check class distribution of original, train, and test sets
round(prop.table(table(dplyr::select(temp_1_transformed, critical_priority), exclude = NULL)), 4) * 100
round(prop.table(table(dplyr::select(train_data, critical_priority), exclude = NULL)), 4) * 100
round(prop.table(table(dplyr::select(test_data, critical_priority), exclude = NULL)), 4) * 100

```



The train data contained 4000 sales, test data 1000 sales for a large dataset. 

The response variable had a balance of 75% for `0` response and 25% for `1` response. This was the result of the distribution of the original column `Order Priority` where 4 values (C, H, M, L) had a part of ~25% each. 

```{r split_regression_large}
temp_3 <- df_50000


# Creating recipe to encode all categorical variables except for a target variable
preprocess_factors <- recipe(critical_priority ~ ., data = temp_3) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = FALSE)

# Fit the preprocessing technique 
preprocess_factors_prep <- prep(preprocess_factors)


# Encode factor variables
temp_3_transformed <- bake(preprocess_factors_prep, new_data = temp_3)
temp_3_transformed <- temp_3_transformed  %>%
  mutate_at(vars(starts_with("item_type"), starts_with("region"), starts_with("sales_channel")), as.factor)

# random seed
set.seed(42)

# 80/20 split of the data set
sample <- sample.split(temp_3_transformed$critical_priority, SplitRatio = 0.8)
train_data_large  <- subset(temp_3_transformed, sample == TRUE)
test_data_large   <- subset(temp_3_transformed, sample == FALSE)

# Check dimenstions of train and test data
dim(train_data_large)
dim(test_data_large)

# Check class distribution of original, train, and test sets
round(prop.table(table(dplyr::select(temp_3_transformed, critical_priority), exclude = NULL)), 4) * 100
round(prop.table(table(dplyr::select(train_data_large, critical_priority), exclude = NULL)), 4) * 100
round(prop.table(table(dplyr::select(test_data_large, critical_priority), exclude = NULL)), 4) * 100

```


The train and test data frame are transformed into vectors and the corresponding matrix (X, Y) for the  k-fold cross-validation.

```{r split_valid}
temp_2 <- df_1000

# random seed
set.seed(42)

# 80/20 split of the data set
sample <- sample.split(temp_2$critical_priority, SplitRatio = 0.8)
df_train  <- subset(temp_2, sample == TRUE)
df_test   <- subset(temp_2, sample == FALSE)

# Transform to vectors and the corresponding matrix
x_train <- model.matrix(critical_priority ~ ., data=df_train)[,-1]
y_train <- df_train[,"critical_priority"] 

x_test <- model.matrix(critical_priority ~ ., data=df_test)[,-1]
y_test <- df_test[,"critical_priority"] 
```


```{r split_valid_large}
temp_4 <- df_50000

# random seed
set.seed(42)


# 80/20 split of the data set
sample <- sample.split(temp_4$critical_priority, SplitRatio = 0.8)
df_train_large  <- subset(temp_4, sample == TRUE)
df_test_large   <- subset(temp_4, sample == FALSE)

# Transform to vectors and the corresponding matrix
x_train_large <- model.matrix(critical_priority ~ ., data=df_train_large)[,-1]
y_train_large <- df_train_large[,"critical_priority"] 

x_test_large <- model.matrix(critical_priority ~ ., data=df_test_large)[,-1]
y_test_large <- df_test_large[,"critical_priority"] 
```

For the Random Forest Model, the datasets without transformations were used.
```{r split_forest_small}

splitIndex <- sample.split(df_1000$critical_priority, SplitRatio = 0.8)
train_data_rf <-subset(df_1000, splitIndex == TRUE)
test_data_rf <- subset(df_1000, splitIndex == FALSE)

```

```{r split_forest_large}

splitIndex <- sample.split(df_50000$critical_priority, SplitRatio = 0.8)
train_data_large_rf <-subset(df_50000, splitIndex == TRUE)
test_data_large_rf <- subset(df_50000, splitIndex == FALSE)

```



## 4 Models

### 4.1 Logistic regression - 1000 Sales

As the first step, we built the generalized linear model based on the transformed training dataset with categorical variables transformed to dummy variables. The variables `total_profit`, `total_cost`, `total_revenue` were dropped as they are connected with each other and derived from `unis_sold` and `units_cost` variables. Variables `item_type_Vegetables`, `item_type_Snacks`, as the model ran error when predicting the probabilities, the rank of the data matrix was at least equal to the number of parameters, it was not. Since our dependent variable takes only two values (0 and 1), we used logistic regression. To do so, the function `glm()` with `family=binomial` was used. 
A positive coefficient meant that as the predictor variable grew, so did the log odds of the outcome occurring, increasing the likelihood of the outcome.
A negative coefficient implied the inverse.

```{r log_model}
# Build logistic regression model
log_model <- glm(critical_priority ~ . -total_profit - total_revenue - total_cost - item_type_Vegetables - item_type_Snacks, data = train_data, binomial(link = "logit"), control = list(maxit = 1000))

summ(log_model)
```

The summary table didn't show any significant variables. The p-value associated with this Chi-Square statistic was 0 which is less than .05, the model could be useful.

The Akaike information criterion (`AIC`) was `926.46` which could be high, there was a need to compare with other models.

The `accuracy` of the model was 71.5%. Based on the confusion matrix, the model didn't perform well in predicting critical priority as it was shown by the precision and recall values for class 1.

The `null deviance` of `921.05` defined how well the target variable could be predicted by a model with only an intercept term.

The `residual deviance` of `880.46` defined how well the target variable can be predicted by our current model that we fit with  predictor variables mentioned above. The lower the value, the better the model could predict the value of the response variable.

`AUC for the ROC curve` is `0.52`.

```{r log_model_predict, warning=FALSE, message=FALSE}

# Predict using test data
log_model_prob <- predict(log_model, newdata = test_data, type = "response")
log_model_pred <- ifelse(log_model_prob  > 0.5, 1, 0) 

# Evaluate the Logistic Regression model
conf_matrix_1 <- confusionMatrix(factor(log_model_pred), factor(test_data$critical_priority), "1")


results <- tibble(Model = "Model #1 - Log Regression 1000 Sales", Accuracy=conf_matrix_1$overall[1], 
                  "Classification error rate" = 1 - conf_matrix_1$overall[1],
                  F1 = conf_matrix_1$byClass[7],
                  Deviance= log_model$deviance, 
                  R2 = 1 - log_model$deviance / log_model$null.deviance,
                  Sensitivity = conf_matrix_1$byClass["Sensitivity"],
                  Specificity = conf_matrix_1$byClass["Specificity"],
                  Precision =  conf_matrix_1$byClass["Precision"],
                  AIC= log_model$aic, 
                  ROC = auc(roc(test_data$critical_priority, log_model_prob)))

conf_matrix_1

```

The feature importance plot below showed units sold and item type household, fruits, cosmetics had the largest positive influence, indicating that sales from these items were more likely to be prioritized.
Item type beverages, on the other hand, had the largest negative influence, indicating that sales from these categories were less likely to be a major priority.

```{r import_log}
# Get col names without target and coeff from model
feature_names <- colnames(train_data)
feature_names <- feature_names[ !feature_names == 'critical_priority']
coefficients <- log_model$coefficients

# Sort coeff in desc order
sort_coef <- order(abs(coefficients), decreasing = TRUE)

# Create the plot
ggplot(data.frame(Feature = feature_names[sort_coef], Coefficient = coefficients[sort_coef]), aes(x = Coefficient, y = reorder(Feature, Coefficient))) +
  geom_col() +
  labs(title = "Feature Importances, Logistic Regression, 1000 Sales", x = "Importance", y = "Feature") +
  theme_minimal()

```


#### Assumptions

The linear link between the independent variables and the logit transformation of the response variable was required by the Logistic regression model. 
The visualization below showed no required linearity between independent variables and the log-odds of the target.


```{r linearity_log}
probabilities <- predict(log_model, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)

#Only numeric predictors
data <- train_data %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(data)

# Bind the logit and tidying the data for plot
data <- data %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

#Scatter plot
ggplot(data, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```


Also, observations had to be independent of each other. As this is a randomly created dataset, we could assume that each sale record was independent.

There had to be no multicollinearity. The correlation matrix below didn't show extremely high multicollinearity. However, variables `total_revenue`, `total_cost`, `total_profit` were related and could be a multicollinearity issue, they were not used in the logistic regression model.


```{r corr_log}

rcore <- rcorr(as.matrix(train_data %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```



### 4.2 Logistic regression - 50000 Sales

The generalized linear model was built based on the transformed training dataset with categorical variables transformed to dummy variables. The variables `total_profit`, `total_cost`, `total_revenue`,`item_type_Vegetables`, `item_type_Snacks`were dropped as in the model for 1000 sales. The same glm() function was used to build the model.

A positive coefficient meant that as the predictor variable grew, so did the log odds of the outcome occurring, increasing the likelihood of the outcome.
A negative coefficient implied the inverse.

```{r log_model_large}
# Build logistic regression model
log_model_large <- glm(critical_priority ~ . -total_profit - total_revenue - total_cost - item_type_Vegetables - item_type_Snacks, data = train_data_large, binomial(link = "logit"), control = glm.control(maxit = 1000))

summ(log_model_large)
```

The summary table didn't show any significant variables. The p-value associated with this Chi-Square statistic was 0 which is less than .05, the model could be useful.

The Akaike information criterion (`AIC`) was `44925.52` which could be high, there was a need to compare with other models.

The `accuracy` of the model was 75.1%, higher than for the 1000 sales. The Logistic Regression model predicted that all occurrences were non-essential, resulting in a critical priority precision, recall, and F1-score of 0%. This result was worse comparing to the result for 1000 sales which indicated a strong class imbalance in the data.

The `null deviance` of `44892.08` defined how well the target variable could be predicted by a model with only an intercept term.

The `residual deviance` of ` 44879.52` showed that our model couldn't predict well the response variable.

```{r log_model_predict_large, warning=FALSE, message=FALSE}

# Predict using test data
log_model_prob_large <- predict(log_model_large, newdata = test_data_large, type = "response")
log_model_pred_large <- ifelse(log_model_prob_large  > 0.5, 1, 0) 

# Evaluate the Logistic Regression model
conf_matrix_2 <- confusionMatrix(factor(log_model_pred_large), factor(test_data_large$critical_priority), "1")


results <- rbind(results, tibble(Model = "Model #2 - Log Regression 50000 Sales", Accuracy=conf_matrix_2$overall[1], 
                  "Classification error rate" = 1 - conf_matrix_2$overall[1],
                  F1 = conf_matrix_2$byClass[7],
                  Deviance= log_model_large$deviance, 
                  R2 = 1 - log_model_large$deviance / log_model_large$null.deviance,
                  Sensitivity = conf_matrix_2$byClass["Sensitivity"],
                  Specificity = conf_matrix_2$byClass["Specificity"],
                  Precision = conf_matrix_2$byClass["Precision"],
                  AIC= log_model_large$aic,
                  ROC = NA)) #auc(roc(test_data$critical_priority, log_model_prob_large)))) 

conf_matrix_2

```

The feature importance plot below showed units sold and item type Beverages, region Sub-Saharan Africa had the largest positive influence, indicating that sales from this item and region were more likely to be prioritized.
Item types cosmetics, fruits, household, on the other hand, had the largest negative influence, indicating that sales from these categories were less likely to be a major priority. These results were almost opposite to the results for 1000 sales.

```{r import_log_large}
# Get col names without target and coeff from model
feature_names <- colnames(train_data_large)
feature_names <- feature_names[ !feature_names == 'critical_priority']
coefficients <- log_model_large$coefficients

# Sort coeff in desc order
sort_coef <- order(abs(coefficients), decreasing = TRUE)

# Create the plot
ggplot(data.frame(Feature = feature_names[sort_coef], Coefficient = coefficients[sort_coef]), aes(x = Coefficient, y = reorder(Feature, Coefficient))) +
  geom_col() +
  labs(title = "Feature Importances, Logistic Regression, 50000 Sales", x = "Importance", y = "Feature") +
  theme_minimal()

```


#### Assumptions

The linear link between the independent variables and the logit transformation of the response variable was required by the Logistic regression model. 
The visualization below showed no required linearity between independent variables and the log-odds of the target.

The code below took too much time to run, the eval was set to FALSE to be able to post the report.
```{r linearity_log_large, eval=FALSE}
probabilities <- predict(log_model_large, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)

#Only numeric predictors
data <- train_data_large %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(data)

# Bind the logit and tidying the data for plot
data <- data %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

#Scatter plot
ggplot(data, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")
```


Also, observations had to be independent of each other. As this is a randomly created dataset, we could assume that each sale record was independent.

There had to be no multicollinearity. The correlation matrix below didn't show extremely high multicollinearity. However, variables `total_revenue`, `total_cost`, `total_profit` were related and could be a multicollinearity issue, they were not used in the logistic regression model.
The correlation between variables looked less than in the small data.


```{r corr_log_large}

rcore <- rcorr(as.matrix(train_data_large %>% dplyr::select(where(is.numeric))))
coeff <- rcore$r
corrplot(coeff, tl.cex = .7, tl.col="black", method = 'color', addCoef.col = "black",
         type="upper", order="hclust",
         diag=FALSE)

```



### 4.2 Cross-validation - 1000 Sales
The the k-fold cross-validation with Lasso regularization was used in order to improve feature selection for the Logistic regression model.

```{r lasso__model}

# Train Lasso rmodel
set.seed(42)
lasso_model<- cv.glmnet(x_train, 
                       y_train,
                       alpha = 1, #alpha=1 is lasso
                       family = "binomial",
                       link = "logit",
                       nfolds = 5, 
                       type.measure = "class")


```

The `accuracy` of the Lasso model is 74% instead of the previous 71.5% in the Logistic model for 1000 sales. The AUC score of 0.5 didn't suggest discriminative power. Based on the confusion matrix, the model didn't perform well in predicting critical priority as it was shown by the precision and recall values for class 1.

The Lasso regularization showed that none of the independent variables were significant for the classification. This could happen due the nature of the synthetic data or the imbalance in the target variable.

The Akaike information criterion (`AIC`) was extremely negative.

```{r}
glmnet_cv_aicc <- function(fit, lambda = 'lambda.min'){
  whlm <- which(fit$lambda == fit[[lambda]])
  with(fit$glmnet.fit,
       {
         tLL <- nulldev - nulldev * (1 - dev.ratio)[whlm]
         k <- df[whlm]
         n <- nobs
         return(list('AICc' = - tLL + 2 * k + 2 * k * (k + 1) / (n - k - 1),
                     'BIC' = log(n) * k - tLL))
       })
}
```

```{r lasso_pred, warning=FALSE, message=FALSE}
# Predict the test data
lasso_prob <- predict(lasso_model, s = lasso_model$lambda.min, newx = x_test, type = "response")
lasso_pred <- ifelse(lasso_prob > 0.5, 1, 0) 


# The features selected by Lasso
features_matrix <- as.matrix(coef(lasso_model, s = lasso_model$lambda.min))
selected_features <- rownames(features_matrix)[features_matrix != 0]

selected_features

# Confusion matrix
conf_matrix_3 = confusionMatrix(factor(lasso_pred),factor(y_test), "1")


lasso.r1 <- assess.glmnet(lasso_model,           
                                newx = x_test,              
                                newy = y_test )   
lasso.r2 <- glmnet_cv_aicc(lasso_model, 'lambda.min')

lasso.r1$auc[1]

conf_matrix_3

results <- rbind(results,tibble(Model = "Model #3 - Lasso Model 1000 Sales", Accuracy=conf_matrix_3$overall[1], 
                  "Classification error rate" = 1 - lasso.r1$auc[1],
                  F1 = conf_matrix_3$byClass[7],
                  Deviance=lasso.r1$deviance[[1]], 
                  R2 = NA,
                  Sensitivity = conf_matrix_3$byClass["Sensitivity"],
                  Specificity = conf_matrix_3$byClass["Specificity"],
                  Precision =  conf_matrix_3$byClass["Precision"],
                  AIC= lasso.r2$AICc, 
                  ROC = auc(roc(y_test, as.numeric(lasso_prob)))))
```

#### Assumptions


The observations had to be independent of each other. As this is a randomly created dataset, we could assume that each sale record was independent

Each fold in the k-fold cross-validation had to have approximately the same proportion of samples of each target class (stratified sampling).


### 4.3 Cross-validation - 50000 Sales

The steps to build the k-fold cross-validation with Lasso regularization were repeated for 50000 sales.

```{r lasso__model_large}

# Train Lasso rmodel
set.seed(42)
lasso_model_large <- cv.glmnet(x_train_large, 
                       y_train_large,
                       alpha = 1, #alpha=1 is lasso
                       family = "binomial",
                       link = "logit",
                       nfolds = 5, 
                       type.measure = "class")


```

The `accuracy` of the Lasso model is 75%, the same as in the model without cross-validation. The AUC score of 0.5 didn't suggest discriminative power. Based on the confusion matrix, the model didn't perform well in predicting critical priority as it was shown by the precision and recall values for class 1, the same as for all the previous regression model.

The Lasso regularization showed that none of the independent variables were significant for the classification. This could happen due the nature of the synthetic data or the imbalance in the target variable.

The Akaike information criterion (`AIC`) was extremely negative.

```{r lasso_pred_large, warning=FALSE, message=FALSE}
# Predict the test data
lasso_prob_large <- predict(lasso_model_large, s = lasso_model_large$lambda.min, newx = x_test_large, type = "response")
lasso_pred_large <- ifelse(lasso_prob_large > 0.5, 1, 0) 


# The features selected by Lasso
features_matrix <- as.matrix(coef(lasso_model_large, s = lasso_model_large$lambda.min))
selected_features <- rownames(features_matrix)[features_matrix != 0]

selected_features

# Confusion matrix
conf_matrix_4 = confusionMatrix(factor(lasso_pred_large),factor(y_test_large), "1")


lasso.r1 <- assess.glmnet(lasso_model_large,           
                                newx = x_test_large,              
                                newy = y_test_large)   
lasso.r2 <- glmnet_cv_aicc(lasso_model_large, 'lambda.min')

lasso.r1$auc[1]

conf_matrix_4

results <- rbind(results,tibble(Model = "Model #4- Lasso Model 50000 Sales", Accuracy=conf_matrix_4$overall[1], 
                  "Classification error rate" = 1 - lasso.r1$auc[1],
                  F1 = conf_matrix_4$byClass[7],
                  Deviance=lasso.r1$deviance[[1]], 
                  R2 = NA,
                  Sensitivity = conf_matrix_4$byClass["Sensitivity"],
                  Specificity = conf_matrix_4$byClass["Specificity"],
                  Precision =  conf_matrix_4$byClass["Precision"],
                  AIC= lasso.r2$AICc,
                  ROC = auc(roc(y_test_large, as.numeric(lasso_prob_large)))))
```

#### Assumptions


The observations had to be independent of each other. As this is a randomly created dataset, we could assume that each sale record was independent

Each fold in the k-fold cross-validation had to have approximately the same proportion of samples of each target class (stratified sampling).

### 4.5 Random Forest - 1000 Sales

Next, we tried  Random Forest Regression model for the small dataset. 

```{r rf_model}
# Build a Random Forest model
rf_model <- randomForest(critical_priority ~ . , data = train_data, ntree = 100, importance = TRUE)
print(rf_model)
```


The `accuracy` of the model was 70%. Comparing to th previous models, the Random forest model better predicted critical priority as it was shown by the precision and recall values for class 1.


```{r rd_pred, warning=FALSE, message=FALSE}
# Predict test data
rf_model_pred <- predict(rf_model, newdata = test_data)

# Evaluate the model

conf_matrix_5 <- confusionMatrix(factor(rf_model_pred ), factor(test_data$critical_priority), "1")


results <- rbind(results, tibble(Model = "Model #5 - Random Forest 1000 Sales", Accuracy=conf_matrix_5$overall[1], 
                  "Classification error rate" = 1 - conf_matrix_5$overall[1],
                  F1 = conf_matrix_5$byClass[7],
                  Deviance= NA, 
                  R2 = NA,
                  Sensitivity = conf_matrix_5$byClass["Sensitivity"],
                  Specificity = conf_matrix_5$byClass["Specificity"],
                  Precision = conf_matrix_5$byClass["Precision"],
                  AIC= NA,
                  ROC = NA))

conf_matrix_5

```

The plot below showed the feature importance, the Mean Decrease Accuracy showed how much the model accuracy decreased when we dropped that variable. Higher the value of mean decrease accuracy or mean decrease gini score, higher the importance of the variable in the model. Units sold was the most important variable.
```{r}
plot(rf_model)
varImpPlot(rf_model, main = "Varibale Importance, Random Forest, 1000 Sales" , pch=16) 
```


#### Assumptions

Although Random Forest Regression didn't have strict assumptions like linear models, there were several things to consider when building the model.

For example, there was no need for a data transformations or the need to follow a specific distribution as non-linear interactions can be captured without any explicit transformation. 


The plot below depicted a single forest decision tree. Each node reflects a decision depending on the value of a feature. The decision seeks to separate the data in such a way that the purity of the resulting child nodes is increased. Because the tree makes binary choices, there are always two branches.
The anticipated class is shown by the color of the leaf nodes, with darker colors signifying better predictions.

```{r}


# Build a single tree 
single_tree <- rpart(critical_priority ~ ., data = train_data, method = "class", control = rpart.control(cp = 0.01))

# Plot the tree
rpart.plot(single_tree)


```





### 4.6 Random Forest - 50000 Sales

As the last model, Random Forest Regression model for the large dataset.

Building the model for the larger dataset took significantly more time compared to the smaller dataset

```{r rf_model_large}
# Build a Random Forest model
rf_model_large <- randomForest(critical_priority ~ . , data = train_data_large, ntree = 100, importance = TRUE)
print(rf_model_large)
```

The `accuracy` of the model was 75%, the same as the previous models for 50000 sales. The Random Forest model predicted certain crucial events, but with low precision and recall.

```{r rd_pred_large, warning=FALSE, message=FALSE}
# Predict test data
rf_model_pred_large <- predict(rf_model_large, newdata = test_data_large)

# Evaluate the model

conf_matrix_6 <- confusionMatrix(factor(rf_model_pred_large), factor(test_data_large$critical_priority), "1")


results <- rbind(results, tibble(Model = "Model #6 - Random Forest 50000 Sales", Accuracy=conf_matrix_6$overall[1], 
                  "Classification error rate" = 1 - conf_matrix_6$overall[1],
                  F1 = conf_matrix_6$byClass[7],
                  Deviance= NA, 
                  R2 = NA,
                  Sensitivity = conf_matrix_6$byClass["Sensitivity"],
                  Specificity = conf_matrix_6$byClass["Specificity"],
                  Precision =NA,
                  AIC= NA,
                  ROC = NA))

conf_matrix_6

```

The plot below showed the feature importance., the Mean Decrease Accuracy showed how much the model accuracy decreased when we dropped that variable. Higher the value of mean decrease accuracy or mean decrease gini score, higher the importance of the variable in the model. Units sold was the most important variable.

```{r }
plot(rf_model_large)
varImpPlot(rf_model_large,  main = "Varibale Importance, Random Forest, 50000 Sales") 
```


#### Assumptions

Although Random Forest Regression didn't have strict assumptions like linear models, there were several things to consider when building the model.

For example, there was no need for a data transformations or the need to follow a specific distribution as non-linear interactions can be captured without any explicit transformation. 


The plot below depicted a single forest decision tree. Each node reflects a decision depending on the value of a feature. The decision seeks to separate the data in such a way that the purity of the resulting child nodes is increased. Because the tree makes binary choices, there are always two branches.
The anticipated class is shown by the color of the leaf nodes, with darker colors signifying better predictions.

```{r }

# Build a single tree
single_tree <- rpart(critical_priority ~ . , data = train_data_large, method = "class", control = rpart.control(cp = 0.01))

# Plot the tree
rpart.plot(single_tree)


```



## 5. Model selection

The performance of Logistic Regression and Random Forest were quite close for both datasets.
The accuracy of both Logistic Regression and Random Forest models was marginally greater for the "50000 Sales Records" dataset than for the "1000 Sales Records" dataset. For both datasets, the performance of Logistic Regression was consistently dismal in terms of critical priorities Precision, and Recall for class 1. The Lasso regularization didn't improve the logistic regression.


The Random Forest model trained on the "50000 Sales Records" dataset had a greater recall for essential priorities than the model trained on the "1000 Sales Records" dataset

Both algorithms struggled to reliably identify essential priorities. This could be due to the imbalance in the classes, the necessity for more complex modeling methodologies (tuning hyperparameters, feature engineering, or using other modeling techniques), and the synthetic nature of the data.

Anyway, the Random Forest Regression looked like a better choice, it could predict some of the class 1 values. There were no requirements for preprocessing tasks such as scaling and normalization of each feature. In reality, the larger datasets can give better results as it is more representative, and models can be trained on more diverse data, making them more robust and less prone to overfitting.


```{r}
nice_table <- function(df, cap=NULL, cols=NULL, dig=3, fw=F){
  if (is.null(cols)) {c <- colnames(df)} else {c <- cols}
  table <- df %>% 
    kable(caption=cap, col.names=c, digits=dig) %>% 
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      html_font = 'monospace',
      full_width = fw)
  return(table)
}

results %>% 
  nice_table(cap='Logistic Model Comparison') %>% 
  scroll_box(width='100%')
```


## 6. Conclusion

The provided datasets were generated using random logic and were not genuine sales data. Since the data was randomly generated, the patterns, connections, and distributions within the data were not indicative of real-world sales circumstances. As a result, any patterns or associations found by the models could be artifacts of the random data generating process rather than real-world occurrences. Both our Logistic Regression and Random Forest models struggled to identify significant priorities in both datasets. This difficulty was caused by the data's inherent randomness, which lacks any underlying patterns that a model could leverage. Models trained on this data should not be used in real-world circumstances. The provided data was not adequate for true predictive modeling applications. The above work could be used for the preprocessing of sales data as it wouldn't lead to costly mistakes on real data. In reality, if I had to make a business choice, I'd choose a model for the dataset with 50,000 sales as it had a larger sample size and was more likely to produce a more generic model. As mentioned above in the results and remembering the pros and cons of both algorithms, I would choose Random Forest Regression.

As a result, the randomly generated data might be valuable for practice and testing, but it is critical to understand its limitations when evaluating results or contemplating its use.


## References

1. Verma, V. A. (2021). Downloads 18 – Sample CSV Files / Data Sets for Testing (till 5 Million Records) – Sales. Excel BI Analytics. https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/

2. Nwanganga, F., & Chapple, M. (2020). _Practical Machine Learning in R._ https://doi.org/10.1002/9781119591542

3. Sheather, S. (2009). _A Modern Approach to Regression with R._ Springer Science & Business Media.

4. Faraway, J. J. (2016). _Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models._ CRC Press.

----------------


## Appendix: Essay. Machine Learning models for Sales Data



We methodically threaded through stages from data preparation to model selection as we embarked on the analytical journey of our sales datasets. This procedure was intended to be methodical as well as enlightening.

The datasets with 1000 sales and 50000 sales, both with 17 predictor variables. Correlations between columns were thoroughly evaluated during the data processing phase. Notably, there was a substantial link between total_revenue, total_cost, and total_profit.

The first critical step was data transformation. The target variable was the `critical_priority` column built from `Order Priority`. The critical orders were substituted with a `1` and all others with a `0`. The datasets were refined further for clarity and efficiency: columns were given standardized names, dates were formatted consistently, and extraneous columns such as  `Country` and `Order ID` were removed. The `Order Date` and `Ship Date` columns were used to create new columns `order_year`, `order_month` and were removed. The datasets were also prepared for algorithmic processing by converting many character columns into model-compatible dummy variables.

As the target variable was binary, the following algorithms were considered. The following data preparation was based on this algorithm selection. The choice of the algorithms wasn't affected by the size of the data, only by the nature of the target variable.

`Logistic regression`: simple, interpretable, and efficient, when the relationship independent variables and log chances are linear, it performs well, but may underperform when the data has complex, non-linear relationships.

`Random Forest`: can capture complicated, non-linear patterns in data, resistant to overfitting, can handle numerical as well as categorical data. But it is more difficult to interpret than simpler models, large datasets can be computationally expensive.

The datasets were divided into training (80%) and testing (20%) sets prior to modeling to ensure unbiased validation and a realistic evaluation of model performance on unknown data. The categorical variables were transformed to dummies for the Logistic models, the data without dummy variables was usedfor Random Forest. For the Lasso model, the training and testing data with all variables (no dummies) was transformed to matrix.

`Logistic Regression - 1000 Sales`
A logistic regression model was built for the smaller dataset of 1,000 sales. Due to their intricate relationship and probable multicollinearity, variables such as total_profit, total_cost, total_revenue, item_type_Vegetables, and item_type_Snacks were omitted. The logistic regression model had a 71.5% accuracy. This model's Akaike information criterion (AIC) was 926.46, indicating room for improvement. The Receiver Operating Characteristic (ROC) curve's area under the curve (AUC) was just 0.52. According to the confusion matrix, the model could not predict important priority well, as evidenced by the precision and recall values for class 1. According to feature importance, sales of domestic goods, fruits, and cosmetics were more likely to be prioritized, whereas beverage sales were not.

`Logistic Regression - 50000 Sales`
For the larger dataset, the same logistic regression model was applied with similar variable exclusions. This model's accuracy was slightly better at 75.1%, but its AIC was significantly higher at 44925.52. Intriguingly, the feature importance differed for this model. Sales from beverages and regions like Sub-Saharan Africa were prioritized, while categories like cosmetics, fruits, and household goods were deprioritized. The model predicted that all occurrences were non-essential, resulting in a critical priority precision, recall, and F1-score of 0%. This result was worse comparing to the result for 1000 sales which indicated a strong class imbalance in the data.

`Cross-validation - 1000 & 50000 Sales`
Lasso regularization was developed to improve feature selection in the logistic regression model. The accuracy improved to 74% for the 1,000 sales dataset and remained at 75% for the 50,000 sales dataset. Notably, according to the Lasso regularization, none of the independent variables were relevant for classification.

`Random Forest - 1000 & 50000 Sales`
Following that, Random Forest, a well-known ensemble learning method, was used. The algorithm attained a 70% accuracy for the 1,000 sales dataset. The larger dataset of 50,000 sales, on the other hand, provided an accuracy of 75%. For both datasets, feature significance plots revealed that 'units sold' was the most influential variable. The precision and recall values for class 1 for both datasets showed that the Random forest model predicted critical priority better than the previous models. The findings for predicting class 1 were better for huge data than for limited data.


The performance of Logistic Regression and Random Forest were quite close for both datasets.
The accuracy of both Logistic Regression and Random Forest models was marginally greater for the "50000 Sales Records" dataset than for the "1000 Sales Records" dataset. For both datasets, the performance of Logistic Regression was consistently dismal in terms of critical priorities Precision, and Recall for class 1. The Lasso regularization didn't improve the logistic regression.

The Random Forest model trained on the "50000 Sales Records" dataset had a greater recall for essential priorities than the model trained on the "1000 Sales Records" dataset

Both algorithms struggled to reliably identify essential priorities. This could be due to the imbalance in the classes, the necessity for more complex modeling methodologies (tuning hyperparameters, feature engineering, or using other modeling techniques), and the synthetic nature of the data.

Anyway, the Random Forest Regression looked like a better choice, it could predict some of the class 1 values. There were no requirements for preprocessing tasks such as scaling and normalization of each feature. In reality, the larger datasets can give better results as it is more representative, and models can be trained on more diverse data, making them more robust and less prone to overfitting.

The provided datasets were generated using random logic and were not genuine sales data. Since the data was randomly generated, the patterns, connections, and distributions within the data were not indicative of real-world sales circumstances. As a result, any patterns or associations found by the models could be artifacts of the random data generating process rather than real-world occurrences. Both our Logistic Regression and Random Forest models struggled to identify significant priorities in both datasets. This difficulty was caused by the data's inherent randomness, which lacks any underlying patterns that a model could leverage. Models trained on this data should not be used in real-world circumstances. The provided data was not adequate for true predictive modeling applications. The above work could be used for the preprocessing of sales data as it wouldn't lead to costly mistakes on real data. In reality, if I had to make a business choice, I'd choose a model for the dataset with 50,000 sales as it had a larger sample size and was more likely to produce a more generic model. As mentioned above in the results and remembering the pros and cons of both algorithms, I would choose Random Forest Regression.

As a result, the randomly generated data might be valuable for practice and testing, but it is critical to understand its limitations when evaluating results or contemplating its use.
